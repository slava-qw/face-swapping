{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-01-30T22:28:27.204614Z",
     "iopub.status.busy": "2025-01-30T22:28:27.204305Z",
     "iopub.status.idle": "2025-01-30T22:28:31.536098Z",
     "shell.execute_reply": "2025-01-30T22:28:31.534995Z",
     "shell.execute_reply.started": "2025-01-30T22:28:27.204590Z"
    },
    "id": "XoU7GZWFxuBC",
    "outputId": "3dc32778-6718-40e3-b820-695d5964a80d",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'One-Shot-Face-Swapping-on-Megapixels'...\n",
      "remote: Enumerating objects: 319, done.\u001b[K\n",
      "remote: Counting objects: 100% (20/20), done.\u001b[K\n",
      "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
      "remote: Total 319 (delta 15), reused 13 (delta 11), pack-reused 299 (from 1)\u001b[K\n",
      "Receiving objects: 100% (319/319), 11.42 MiB | 41.18 MiB/s, done.\n",
      "Resolving deltas: 100% (108/108), done.\n",
      "/kaggle/working/One-Shot-Face-Swapping-on-Megapixels/inference\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (1.11.1.3)\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/zyainfal/One-Shot-Face-Swapping-on-Megapixels.git\n",
    "%cd /kaggle/working/One-Shot-Face-Swapping-on-Megapixels/inference\n",
    "\n",
    "!git clone https://github.com/rosinality/stylegan2-pytorch.git\n",
    "!cp -r /kaggle/working/One-Shot-Face-Swapping-on-Megapixels/inference/stylegan2-pytorch/* /kaggle/working/One-Shot-Face-Swapping-on-Megapixels/inference/\n",
    "!pip install ninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-01-30T22:28:37.982295Z",
     "iopub.status.busy": "2025-01-30T22:28:37.981927Z",
     "iopub.status.idle": "2025-01-30T22:28:38.605247Z",
     "shell.execute_reply": "2025-01-30T22:28:38.604254Z",
     "shell.execute_reply.started": "2025-01-30T22:28:37.982267Z"
    },
    "id": "1MvrjVzi3ddD",
    "outputId": "e37c4e90-7b85-44a5-dfdd-d01ec51f8327",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working\n",
      "fatal: destination path 'stylegan2-pytorch' already exists and is not an empty directory.\n",
      "/kaggle/working/stylegan2-pytorch\n",
      "Cloning into 'stylegan2'...\n",
      "remote: Enumerating objects: 138, done.\u001b[K\n",
      "remote: Total 138 (delta 0), reused 0 (delta 0), pack-reused 138 (from 1)\u001b[K\n",
      "Receiving objects: 100% (138/138), 594.93 KiB | 11.67 MiB/s, done.\n",
      "Resolving deltas: 100% (64/64), done.\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working/\n",
    "!git clone https://github.com/rosinality/stylegan2-pytorch.git\n",
    "%cd /kaggle/working/stylegan2-pytorch\n",
    "!git clone https://github.com/NVlabs/stylegan2.git\n",
    "\n",
    "!cp -r /kaggle/working/stylegan2-pytorch/stylegan2/* /kaggle/working/stylegan2-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "8c0fbc34e4dd49629adf6cb1eed1b7e3",
      "21402ee1741343bca80828125fb32c27",
      "79f20b3631da4926b710ae4a20b6e35e",
      "51bfe958033144dfb3ca5de86c0cf182",
      "744679683d8f4d0e8b6f99b17697b494",
      "615b390994bc49d5b8c22bc25670a407",
      "a9caf07522ec426a8f239b71d0fe7784",
      "1bfb715736ce4a72872db501dde62654",
      "d3107bcf9c404e88abf70c78b6dc64d6",
      "563f47c686ed4f879cb46976ab969c2a",
      "69893729dff44be286d7a9878e995ca4"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-01-30T22:28:44.619377Z",
     "iopub.status.busy": "2025-01-30T22:28:44.619031Z",
     "iopub.status.idle": "2025-01-30T22:28:47.124422Z",
     "shell.execute_reply": "2025-01-30T22:28:47.123720Z",
     "shell.execute_reply.started": "2025-01-30T22:28:44.619345Z"
    },
    "id": "F4dNAhtBEUf2",
    "outputId": "edc53f12-1199-425c-c3b2-bee49de29a70",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1babc23255ba444fa07403f5761cd803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "stylegan2-ffhq-config-f.pt:   0%|          | 0.00/381M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "weight_path = hf_hub_download(repo_id=\"akhaliq/OneshotCLIP-stylegan2-ffhq\", filename=\"stylegan2-ffhq-config-f.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T22:30:03.503740Z",
     "iopub.status.busy": "2025-01-30T22:30:03.503409Z",
     "iopub.status.idle": "2025-01-30T22:30:29.710816Z",
     "shell.execute_reply": "2025-01-30T22:30:29.709790Z",
     "shell.execute_reply.started": "2025-01-30T22:30:03.503685Z"
    },
    "id": "UVNkNrgLEoC1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cmd = f\"!cp {weight_path} /kaggle/working/One-Shot-Face-Swapping-on-Megapixels/inference/checkpoint/\"\n",
    "!${cmd}\n",
    "\n",
    "os.rename(\"/kaggle/working/One-Shot-Face-Swapping-on-Megapixels/inference/checkpoint/stylegan2-ffhq-config-f.pt\",\n",
    "          \"/kaggle/working/One-Shot-Face-Swapping-on-Megapixels/inference/checkpoint/stylegan2-ffhq-config-f.pth\"\n",
    "          )\n",
    "\n",
    "!cp \"/kaggle/input/ftm-model/ftm_final.pth\" /kaggle/working/One-Shot-Face-Swapping-on-Megapixels/inference/checkpoint/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iiShot-1OSBJ",
    "outputId": "636f5def-93e8-413b-eeef-6d1147d27292",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# %cd /kaggle/working/\n",
    "# !gdown 1badu11NqxGf6qM3PTTooQDJvQbejgbTv\n",
    "# !unzip /content/CelebAMask-HQ.zip\n",
    "\n",
    "# !unzip /content/drive/MyDrive/Colab_Notebooks/CV/gasn/data/CelebAMask-HQ.zip\n",
    "# !unzip /content/drive/MyDrive/Colab_Notebooks/CV/gasn/data/CelebA-HQ-masks.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IkDtBUXoOR_x"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D6IGmY4zOR3X"
   },
   "outputs": [],
   "source": [
    "# from google.colab import userdata\n",
    "# import os\n",
    "\n",
    "# os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')\n",
    "# os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\n",
    "\n",
    "# !kaggle datasets download -d lamsimon/celebahq\n",
    "# !unzip \"/content/celebahq.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "execution": {
     "iopub.execute_input": "2025-01-30T23:02:45.458674Z",
     "iopub.status.busy": "2025-01-30T23:02:45.458311Z",
     "iopub.status.idle": "2025-01-30T23:04:04.543969Z",
     "shell.execute_reply": "2025-01-30T23:04:04.543005Z",
     "shell.execute_reply.started": "2025-01-30T23:02:45.458645Z"
    },
    "id": "8kZe8IGrdlk-",
    "outputId": "2f55e333-640c-47bc-d260-fc533ed383d5",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/One-Shot-Face-Swapping-on-Megapixels/inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%cd /kaggle/working/One-Shot-Face-Swapping-on-Megapixels/inference\n",
    "from inference import MegaFS\n",
    "\n",
    "\n",
    "str2num = {\n",
    "    # 0: 'background',\n",
    "    1: 'skin',\n",
    "    2: 'l_brow',\n",
    "    3: 'r_brow',\n",
    "    4: 'l_eye',\n",
    "    5: 'r_eye',\n",
    "    6: 'eye_g',\n",
    "    7: 'l_ear',\n",
    "    8: 'r_ear',\n",
    "    # 9: 'ear_r',\n",
    "    10: 'nose',\n",
    "    11: 'mouth',\n",
    "    12: 'u_lip',\n",
    "    13: 'l_lip',\n",
    "    14: 'neck',\n",
    "    # 15: 'neck_l',\n",
    "    # 16: 'cloth',\n",
    "    17: 'hair',\n",
    "    # 18: 'hat'\n",
    "}\n",
    "\n",
    "\n",
    "class My_MegaFS(MegaFS):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def read_pair(self, src_idx, tgt_idx):\n",
    "        src_face = cv2.imread(os.path.join(self.img_root, \"{}.jpg\".format(src_idx)))\n",
    "        tgt_face = cv2.imread(os.path.join(self.img_root, \"{}.jpg\".format(tgt_idx)))\n",
    "        tgt_mask  = cv2.imread(os.path.join(self.mask_root, \"{}_mask.png\".format(tgt_idx)))\n",
    "\n",
    "        src_face_rgb = src_face[:, :, ::-1]\n",
    "        tgt_face_rgb = tgt_face[:, :, ::-1]\n",
    "        # tgt_mask = self.encode_segmentation_rgb(tgt_idx)\n",
    "        return src_face_rgb, tgt_face_rgb, tgt_mask\n",
    "\n",
    "\n",
    "    def run(self, src_idx, tgt_idx, refine=False, save_path='/content'):\n",
    "        src_face_rgb, tgt_face_rgb, tgt_mask = self.read_pair(src_idx, tgt_idx)\n",
    "        source, target = self.preprocess(src_face_rgb, tgt_face_rgb)\n",
    "        swapped_face = self.swap(source, target)\n",
    "        swapped_face = self.postprocess(swapped_face, tgt_face_rgb, tgt_mask)\n",
    "\n",
    "        result = np.hstack((src_face_rgb[:,:,::-1], tgt_face_rgb[:,:,::-1], swapped_face))\n",
    "\n",
    "        if refine: # True give some bad results...\n",
    "            swapped_tensor, _ = self.preprocess(swapped_face[:,:,::-1], swapped_face)\n",
    "            refined_face = self.refine(swapped_tensor)\n",
    "            refined_face = self.postprocess(refined_face, tgt_face_rgb, tgt_mask)\n",
    "            result = np.hstack((result, refined_face))\n",
    "\n",
    "        save_path = f\"{save_path}/{self.swap_type}_{src_idx}_and_{tgt_idx}\"\n",
    "        print(save_path)\n",
    "        cv2.imwrite(\"{}.jpg\".format(save_path), result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ULZYPvBOyJks",
    "outputId": "62336b26-c8ce-4fa9-b820-cf69d4c63625"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load encoder & swapper: ./checkpoint/ftm_final.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/One-Shot-Face-Swapping-on-Megapixels/inference/inference.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpts = torch.load(ckpt_e, map_location=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load generator: ./checkpoint/stylegan2-ffhq-config-f.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/One-Shot-Face-Swapping-on-Megapixels/inference/inference.py:88: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpts = torch.load(ckpt_f, map_location=torch.device(\"cpu\"))\n",
      "/usr/local/lib/python3.11/dist-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "# handler = My_MegaFS(swap_type=\"ftm\",\n",
    "#                  img_root=\"/kaggle/input/celebamask-hq/CelebAMask-HQ/CelebA-HQ-img\",\n",
    "#                  mask_root=\"/kaggle/input/celeba-hq-masks/content/masks_output\"\n",
    "#             )\n",
    "# # they have different sizes: masked 512x512, imgs 1024x1024 (???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "xzNBPpxbuTU5",
    "outputId": "de0ad0b0-daf4-43ac-f8b7-7fde4a3018c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: (1024, 1024, 3) (1024, 1024, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/One-Shot-Face-Swapping-on-Megapixels/inference/megafs.py:270: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n",
      "  p2 = self.output2(c2) + F.upsample(p3, scale_factor=2, mode='bilinear', align_corners=True) # N, 512, 16, 16\n",
      "/content/One-Shot-Face-Swapping-on-Megapixels/inference/megafs.py:271: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n",
      "  p1 = self.output1(c1) + F.upsample(p2, scale_factor=2, mode='bilinear', align_corners=True) # N, 512, 32, 32\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) /io/opencv/modules/imgproc/src/resize.cpp:4152: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-2115daca11fb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"10705\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"10705\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# src_face_rgb, tgt_face_rgb, swapped_face, refined_swapped_face (somewhy has a bad quality)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-122-95e0dae076f1>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, src_idx, tgt_idx, refine, save_path)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_face_rgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_face_rgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mswapped_face\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mswapped_face\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mswapped_face\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_face_rgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_face_rgb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_face_rgb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswapped_face\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-122-95e0dae076f1>\u001b[0m in \u001b[0;36mpostprocess\u001b[0;34m(self, swapped_face, target, target_mask)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswapped_face\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswapped_face\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mtarget_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mmask_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.10.0) /io/opencv/modules/imgproc/src/resize.cpp:4152: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n"
     ]
    }
   ],
   "source": [
    "# handler.run(src_idx=\"10705\", tgt_idx=\"10705\")  # src_face_rgb, tgt_face_rgb, swapped_face, refined_swapped_face (somewhy has a bad quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m5GE0ICI0WKq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U0n9dIrqAQi_"
   },
   "source": [
    "## optimized execution for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "execution": {
     "iopub.execute_input": "2025-01-30T23:44:21.250653Z",
     "iopub.status.busy": "2025-01-30T23:44:21.250335Z",
     "iopub.status.idle": "2025-01-30T23:44:21.263275Z",
     "shell.execute_reply": "2025-01-30T23:44:21.262099Z",
     "shell.execute_reply.started": "2025-01-30T23:44:21.250625Z"
    },
    "id": "XyvDJZMYAQRk",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.functional as tF\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "\n",
    "class FaceSwapDataset(Dataset):\n",
    "    def __init__(self, img_root, mask_root, pairs):\n",
    "        self.img_root = img_root\n",
    "        self.mask_root = mask_root\n",
    "        self.pairs = pairs  # List of (src_idx, tgt_idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_idx, tgt_idx = self.pairs[idx]\n",
    "\n",
    "        src_face = cv2.imread(os.path.join(self.img_root, f\"{src_idx}.jpg\"))\n",
    "        tgt_face = cv2.imread(os.path.join(self.img_root, f\"{tgt_idx}.jpg\"))\n",
    "\n",
    "        src_face_rgb = cv2.cvtColor(src_face, cv2.COLOR_BGR2RGB)\n",
    "        tgt_face_rgb = cv2.cvtColor(tgt_face, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        tgt_idx = \"0\" * (5 - len(str(tgt_idx))) + str(tgt_idx)\n",
    "        tgt_mask = cv2.imread(os.path.join(self.mask_root, f\"{tgt_idx}_mask.png\"))\n",
    "\n",
    "        # https://discuss.pytorch.org/t/negative-strides-in-tensor-error/134287/2\n",
    "        src_face_rgb = np.ascontiguousarray(src_face[:, :, ::-1])  # Copy ensures no negative strides\n",
    "        tgt_face_rgb = np.ascontiguousarray(tgt_face[:, :, ::-1])\n",
    "        tgt_mask = np.ascontiguousarray(tgt_mask)\n",
    "\n",
    "        src_face_rgb_ = cv2.resize(src_face_rgb.copy(), (256, 256))\n",
    "        tgt_face_rgb_ = cv2.resize(tgt_face_rgb.copy(), (256, 256))\n",
    "        src = torch.from_numpy(src_face_rgb_.transpose((2, 0, 1))).float().mul_(1/255.0)\n",
    "        tgt = torch.from_numpy(tgt_face_rgb_.transpose((2, 0, 1))).float().mul_(1/255.0)\n",
    "\n",
    "        src = tF.normalize(src, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5), False)\n",
    "        tgt = tF.normalize(tgt, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5), False)\n",
    "\n",
    "        return src_idx, tgt_idx, src_face_rgb, tgt_face_rgb, tgt_mask, src, tgt\n",
    "\n",
    "\n",
    "def process_batch(batch, model, save_path):\n",
    "    src_idx, tgt_idx, src_face_rgb, tgt_face_rgb, tgt_mask, source, target = batch\n",
    "    # source, target = model.preprocess(src_face_rgb, tgt_face_rgb)\n",
    "\n",
    "    swapped_face = model.swap(source, target)\n",
    "    swapped_face = model.postprocess(swapped_face, tgt_face_rgb.numpy(), ~tgt_mask.squeeze(0).numpy())\n",
    "\n",
    "    # result = np.hstack((src_face_rgb.numpy()[:, :, ::-1], tgt_face_rgb.numpy()[:, :, ::-1], swapped_face))\n",
    "    # print(f\"{result.shape=}\")\n",
    "    save_file = f\"{save_path}/{model.swap_type}_src_{src_idx[0]}__tgt_{tgt_idx[0]}.jpg\"\n",
    "    cv2.imwrite(save_file, swapped_face[0][:, :, ::-1])\n",
    "\n",
    "\n",
    "def run_in_batches(handler, img_root, mask_root, pairs, batch_size=1, num_workers=4, save_path=\"/kaggle/working/results\"):\n",
    "    assert batch_size == 1, \"Batch size must be 1\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    dataset = FaceSwapDataset(img_root, mask_root, pairs)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "        process_batch(batch, handler, save_path)\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "execution": {
     "iopub.execute_input": "2025-01-30T23:04:04.564054Z",
     "iopub.status.busy": "2025-01-30T23:04:04.563672Z",
     "iopub.status.idle": "2025-01-30T23:04:04.586978Z",
     "shell.execute_reply": "2025-01-30T23:04:04.585933Z",
     "shell.execute_reply.started": "2025-01-30T23:04:04.564021Z"
    },
    "id": "OJNYCvgVEkEO",
    "outputId": "f029a691-8d96-41ed-df9a-4906101d1684",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/One-Shot-Face-Swapping-on-Megapixels/inference\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working/One-Shot-Face-Swapping-on-Megapixels/inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "execution": {
     "iopub.execute_input": "2025-01-30T23:04:04.589218Z",
     "iopub.status.busy": "2025-01-30T23:04:04.588880Z",
     "iopub.status.idle": "2025-01-30T23:04:11.850068Z",
     "shell.execute_reply": "2025-01-30T23:04:11.849125Z",
     "shell.execute_reply.started": "2025-01-30T23:04:04.589186Z"
    },
    "id": "ubdYjwuCAVc1",
    "outputId": "eae9fa66-973b-4427-b11c-5040233d0d1a",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load encoder & swapper: ./checkpoint/ftm_final.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kaggle/working/One-Shot-Face-Swapping-on-Megapixels/inference/inference.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpts = torch.load(ckpt_e, map_location=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load generator: ./checkpoint/stylegan2-ffhq-config-f.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kaggle/working/One-Shot-Face-Swapping-on-Megapixels/inference/inference.py:88: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpts = torch.load(ckpt_f, map_location=torch.device(\"cpu\"))\n",
      "/usr/local/lib/python3.10/dist-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "img_root = \"/kaggle/input/celebamask-hq/CelebAMask-HQ/CelebA-HQ-img\"\n",
    "mask_root = \"/kaggle/input/celeba-hq-masks/content/masks_output\"\n",
    "\n",
    "handler = My_MegaFS(swap_type=\"ftm\",\n",
    "                    img_root=img_root,\n",
    "                    mask_root=mask_root)\n",
    "\n",
    "# # Generate all pairs (example: pairing images with themselves)\n",
    "pairs = [(str(i), str(i)) for i in range(30000)]\n",
    "rd1 = random.sample(range(0, 30000), 10000)\n",
    "rd2 = random.sample(range(0, 30000), 10000)\n",
    "\n",
    "pairs = [(str(i), str(j)) for i, j in zip(rd1, rd2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T23:44:33.020412Z",
     "iopub.status.busy": "2025-01-30T23:44:33.020040Z",
     "iopub.status.idle": "2025-01-31T00:43:57.850051Z",
     "shell.execute_reply": "2025-01-31T00:43:57.849028Z",
     "shell.execute_reply.started": "2025-01-30T23:44:33.020372Z"
    },
    "id": "SSo9QcfkFzgM",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e1416787a9d4924bff1b426bb494034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_in_batches(handler, img_root, mask_root, pairs, batch_size=1, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T01:47:19.304725Z",
     "iopub.status.busy": "2025-01-31T01:47:19.304389Z",
     "iopub.status.idle": "2025-01-31T01:47:23.374160Z",
     "shell.execute_reply": "2025-01-31T01:47:23.373332Z",
     "shell.execute_reply.started": "2025-01-31T01:47:19.304683Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/One-Shot-Face-Swapping-on-Megapixels/inference/batch_v2_1.zip'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive('batch_v2_1', 'zip', '/kaggle/working/batch_v2_1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T01:41:52.645165Z",
     "iopub.status.busy": "2025-01-31T01:41:52.644781Z",
     "iopub.status.idle": "2025-01-31T01:41:52.651051Z",
     "shell.execute_reply": "2025-01-31T01:41:52.650323Z",
     "shell.execute_reply.started": "2025-01-31T01:41:52.645130Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='/kaggle/working/gen_10000_b1.zip' target='_blank'>/kaggle/working/gen_10000_b1.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/gen_10000_b1.zip"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink(\"/kaggle/working/gen_10000_b1.zip\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "A1NYD3YszAGZ",
    "XvPO-n4JBYcG",
    "5goK2SklD47E"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6576569,
     "sourceId": 10621634,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6576645,
     "sourceId": 10621767,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6576665,
     "sourceId": 10621814,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6576937,
     "sourceId": 10622321,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6577026,
     "sourceId": 10622458,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6577155,
     "sourceId": 10622650,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6577162,
     "sourceId": 10622663,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6577185,
     "sourceId": 10622713,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1bfb715736ce4a72872db501dde62654": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "21402ee1741343bca80828125fb32c27": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_615b390994bc49d5b8c22bc25670a407",
      "placeholder": "​",
      "style": "IPY_MODEL_a9caf07522ec426a8f239b71d0fe7784",
      "value": "stylegan2-ffhq-config-f.pt: 100%"
     }
    },
    "51bfe958033144dfb3ca5de86c0cf182": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_563f47c686ed4f879cb46976ab969c2a",
      "placeholder": "​",
      "style": "IPY_MODEL_69893729dff44be286d7a9878e995ca4",
      "value": " 381M/381M [00:02&lt;00:00, 107MB/s]"
     }
    },
    "563f47c686ed4f879cb46976ab969c2a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "615b390994bc49d5b8c22bc25670a407": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "69893729dff44be286d7a9878e995ca4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "744679683d8f4d0e8b6f99b17697b494": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "79f20b3631da4926b710ae4a20b6e35e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1bfb715736ce4a72872db501dde62654",
      "max": 381462551,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d3107bcf9c404e88abf70c78b6dc64d6",
      "value": 381462551
     }
    },
    "8c0fbc34e4dd49629adf6cb1eed1b7e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_21402ee1741343bca80828125fb32c27",
       "IPY_MODEL_79f20b3631da4926b710ae4a20b6e35e",
       "IPY_MODEL_51bfe958033144dfb3ca5de86c0cf182"
      ],
      "layout": "IPY_MODEL_744679683d8f4d0e8b6f99b17697b494"
     }
    },
    "a9caf07522ec426a8f239b71d0fe7784": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d3107bcf9c404e88abf70c78b6dc64d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
